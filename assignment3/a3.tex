\documentclass{article}

\usepackage{fullpage}
\usepackage{color}
\usepackage{amsmath}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{parskip}
\usepackage{amssymb}
\usepackage{nicefrac}
\usepackage{listings} % For displaying code
\usepackage{algorithm2e} % pseudo-code

% Answers
\def\ans#1{\par\gre{Answer: #1}}
%\def\ans#1{} % Comment this line to produce document with answers

% Colors
\definecolor{blu}{rgb}{0,0,1}
\def\blu#1{{\color{blu}#1}}
\definecolor{gre}{rgb}{0,.5,0}
\def\gre#1{{\color{gre}#1}}
\definecolor{red}{rgb}{1,0,0}
\def\red#1{{\color{red}#1}}
\def\norm#1{\|#1\|}

% Math
\def\R{\mathbb{R}}
\def\argmax{\mathop{\rm arg\,max}}
\def\argmin{\mathop{\rm arg\,min}}
\newcommand{\mat}[1]{\begin{bmatrix}#1\end{bmatrix}}
\newcommand{\alignStar}[1]{\begin{align*}#1\end{align*}}
\def\half{\frac 1 2}

% LaTeX
\newcommand{\fig}[2]{\includegraphics[width=#1\textwidth]{a3f/#2}}
\newcommand{\centerfig}[2]{\begin{center}\includegraphics[width=#1\textwidth]{a3f/#2}\end{center}}
\newcommand{\matCode}[1]{\lstinputlisting[language=Matlab]{a3f/#1.m}}
\def\items#1{\begin{itemize}#1\end{itemize}}
\def\enum#1{\begin{enumerate}#1\end{enumerate}}


\begin{document}

\title{CPSC 340 Assignment 3 (due Monday October 17 at 11:55pm)}
\author{}
\date{}
\maketitle
\vspace{-4em}


\section{More Unsupervised Learning}


\subsection{Vector Quantization}


Discovering object groups is one motivation for clustering. Another motivation is \emph{vector quantization}, where we find a prototype point for each cluster and replace points in the cluster by their prototype. If our inputs are images, we could use vector quantization on the set of RGB pixel values as a simple image compression algorithm.

Your task is to implement this simple image compression algorithm by writing a \texttt{quantizeImage} and a \texttt{deQuantizeImage} function. The \texttt{quantizeImage} function should take the name of an image file (like ``dog.png'' for the provided image) and a number $b$ as input. It should use the pixels in the image as examples and the 3 colour channels as features, and run $k$-means clustering on this data with $2^b$ clusters. The code should store the cluster means and return four arguments: the cluster assignments $y$, the means $W$, the number of rows in the image $nRows$, and the number of columns $nCols$. The \texttt{deQuantizeImage} function should take these four arguments and return a version of the image (the same size as the original) where each pixel's original colour is replaced with the nearest prototype colour.

To understand why this is compression, consider the original image space. Say the image can take on the values $0,1,\ldots,254,255$ in each colour channel. Since $2^8=256$ this means we need 8 bits to represent each colour channel, for a total of 24 bits per pixel. Using our method, we are restricting each pixel to only take on one of $2^b$ colour values. In other words, we are compressing each pixel from a 24-bit colour representation to a $b$-bit colour representation by picking the $2^b$ prototype colours that are ``most representative'' given the content of the image. So, for example, if $b=6$ then we have 4x compression.

Note: the script \emph{example\_image.jl} shows how to read an image file using \emph{Images} package, how to display an image using the \emph{Plots} package, and how to convert images to/from the feature representation of their pixels.

\blu{\enum{
\item Hand in your \emph{quantizeImage} and \emph{deQuantizeImage} functions.
\pagebreak
\item Show the image obtained if you encode the colours using $1$, $2$, $4$, and $6$ bits per pixel (instead of the original 24-bits).
\item Save the image with 6 bits per pixel to a .PNG file with \emph{save} function. By going from 8-bits to 6-bits to store the colour of each pixel we would expect the PNG file to be 25\% smaller than the original image. Explain why you think the actual PNG file is larger or smaller than this compression value.
\ans{I would expect for the actual PNG to be smaller. As the magnitude of compression is on an exponential basis, the number of bits to represent each pixel actually decreases by 4x and not just 25\%.}
}
}


\pagebreak

\subsection{Density-Based Clustering}

If you run the function \emph{example\_dbCluster}, it will apply the basic density-based clustering algorithm to the dataset from the previous part. The final output should look like this:\\
\fig{.49}{density}\fig{.49}{density2}\\
(The right plot is zoomed in to show the non-outlier part of the data.)
Even though we know that each object was generated from one of four clusters (and we have 4 outliers), the algorithm finds 6 clusters and does not assign some of the original non-outlier objects to any cluster (points not assigned to any cluster and displayed as small black circles). However, the clusters will change if we change the parameters of the algorithm. Find and report values for the two parameters (\emph{radius} and \emph{minPts}) such that the density-based clustering method finds:
\blu{\enum{
\item The 4 ``true" clusters.
\ans{radius = 1, minPts = 3}
\item 3 clusters (merging the top two, which also seems like a reasonable interpretaition).
\ans{radius = 5, minPts = 5}
\item 2 clusters.
\ans{radius = 15, minPts = 15}
\item 1 cluster (consisting of the non-outlier points).
\ans{radius = 20, minPts = 20}
}}


\pagebreak



\section{Matrix Notation and Linear Regression}

\subsection{Converting to Matrix/Vector/Norm Notation}

Using our standard supervised learning notation ($X$, $y$, $w$)
express the following functions in terms of vectors, matrices, and norms (there should be no summations or maximums).
\blu{\enum{
\item $\sum_{i=1}^n  |w^Tx_i - y_i| + \lambda \sum_{j=1}^{d} |w_j|$.
\ans{$\norm{Xw-y}_1 + \lambda \norm{w}_1$}
\item $\sum_{i=1}^n v_i (w^Tx_i - y_i)^2 + \sum_{j=1}^{d} \lambda_j w_j^2$.
\ans{$(Xw-y)^T V (Xw-y) + w^T \Lambda w$}
\item $\left(\max_{i \in \{1,2,\dots,n\}} |w^Tx_i - y_i|\right)^2 +  \half\sum_{j=1}^{d} \lambda_j|w_j|$.
\ans{$(\norm{Xw-y}_\infty)^2 + \half \norm{\Lambda w}_1$}
}}
You can use $V$ to denote a diagonal matrix that has the (non-negative) ``weights'' $v_i$ along the diagonal. The value $\lambda$ (the ``regularization parameter'') is a non-negative scalar. You can $\Lambda$ as a diagonal matrix that has the (non-negative) $\lambda_j$ values along the diagonal.


\pagebreak

\subsection{Minimizing Quadratic Functions as Linear Systems}

Write finding a minimizer $w$ of the functions below as a system of linear equations (using vector/matrix notation and simplifying as much as possible). Note that all the functions below are convex  so finding a $w$ with $\nabla f(w) = 0$ is sufficient to minimize the functions (but show your work in getting to this point).

\blu{\enum{
\item $f(w) = \frac{1}{2}\norm{w-u}^2$ (projection of $u$ onto real space).
\ans{$f(w) = \half\sum_{j=1}^{d} (w_j - u_j)^2 \\
\\
\nabla f(w) = w-u$}
\item $f(w)= \frac{1}{2}\sum_{i=1}^n v_i (w^Tx_i - y_i)^2 + \lambda w^Tu$ (weighted and tilted least squares).
\ans{$f(w) = \half v_1(w_1 x_11 + ... + w_d x_1d - y_1)^2 + ... + \half v_n(w_1 x_n1 + ... + w_d x_nd - y_n)^2  + \lambda u_1 + ... + \lambda u_d \\
\\
\nabla f(w) = \begin{bmatrix}
v_1 (w_1 x_11 + ... + w_d x_1d - y_1) (x_11) + ... + v_n (w_1 x_n1 + ... + w_d x_nd - y_n) (x_n1) + \lambda u_1 \\
... \\
v_1 (w_1 x_11 + ... + w_d x_1d - y_1) (x_1d) + ... + v_n (w_1 x_n1 + ... + w_d x_nd - y_n) (x_nd) + \lambda u_d \\
\end{bmatrix} \\\\
\nabla f(w) = X^TV(Xw-y) + \lambda u$}
\item $f(w) = \frac{1}{2}\norm{Xw - y}^2 + \frac{\lambda}{2}\norm{w-w^0}^2$ (least squares shrunk towards non-zero $w^0$).
\ans{$f(w) = \half (w_1 x_11 + ... + w_d x_1d - y_1)^2 + ... + \half (w_1 x_n1 + ... + w_d x_nd - y_n)^2 + \frac{\lambda}{2}(w_1 - w^0_1)^2 + ... + \frac{\lambda}{2}(w_2 - w^0_2)^2\\
\\
\nabla f(w) = \begin{bmatrix}
(w_1 x_11 + ... + w_d x_1d - y_1) (x_11) + ... + (w_1 x_n1 + ... + w_d x_nd - y_n) (x_n1) + \lambda(w_1 - w^0_1) \\
... \\
(w_1 x_11 + ... + w_d x_1d - y_1) (x_1d) + ... + (w_1 x_n1 + ... + w_d x_nd - y_n) (x_nd) + \lambda(w_d - w^0_d) \\
\end{bmatrix} \\\\
\nabla f(w) = X^T(Xw-y) + \lambda (w-w_0)$}
}}
Above we assume that $u$ and $w^0$ are $d$ by $1$ vectors, that $v$ is a $n$ by $1$ vector. You can use $V$ as a diagonal matrix containing the $v_i$ values along the diagonal.

Hint: Once you convert to vector/matrix notation, you can use the results from class to quickly compute these quantities term-wise.
As a sanity check for your derivation, make sure that your results have the right dimensions. As a sanity check, make that the dimensions match for all quantities/operations. In order to make the dimensions match you may need to introduce an identity matrix. For example, $X^TXw + \lambda w$ can be re-written as $(X^TX + \lambda I)w$.

\pagebreak

\subsection{Convex Functions}

Recall that convex loss functions are typically easier to minimize than non-convex functions, so it's important to be able to identify whether a function is convex.

\blu{Show that the following functions are convex}:

\enum{
\item $f(w) = \half w^2 + w^{-1}$ with $w > 0$.
\ans{$f'(w) = w - w^-2\\
f''(w) = 1 + 2w^3$\\
As $w > 0$, this would mean that $f''(w) > 0$, making the function convex.}
\item $f(w) = \max_i w_i$ with $w \in \R^n$ (maximum).
\ans{By differentiating each $w_i$, it would show that each $w_i$ are individually convex. \\
As such as $f(w)$ is the max of convex functions, that would make $f(w)$ convex too.}
\item $f(y) = \max(0,1-t\cdot y)$ with $y\in \R$ and $t\in\{-1,+1\}$ (hinge loss).
\ans{Let $h(y) = 0, g(y) = 1 - y, k(y) = 1 + y$ \\
As they are all straight line equations, and their second derivative is $\geq 0$, they are all convex functions.\\
Since $f(y)$ is the max of these convex functions, $f(y)$ is convex too.}
\item $f(w) = \norm{Xw-y}^2 + \lambda\norm{w}_1$ with $w \in \R^d, \lambda \geq 0$ (L1-regularized least squares).
\ans{Let $h(w) = \norm{Xw-y}^2$ and $g(w) = \lambda\norm{w}_1$\\\\
$h(w)$ is convex as all squared norms are convex functions. \\
$g(w)$ is convex as $\norm{w}_1$ is a convex function multiplied by a scalar constant greater than 0. \\
As $f(w) = h(w) + g(w)$, it is a sum of convex functions, thus, $f(w)$ is also convex.}
\item $f(w) = \sum_{i=1}^n \log(1+\exp(-y_iw^Tx_i)) $ with $w \in \R^d$ (logistic regression).
\ans{Let $h(z) = \log(1+\exp(z)) \\
\\
h'(z) = \frac{\exp(z)}{1+\exp(z)} \\
h''(z) = \frac{\exp(z)}{(\exp(z) + 1)^2}$, \\
\\
As $h''(z) \geq 0$ for all z, $h(z)$ is convex. \\
\\
Let $g(w) =  h(-y_i w^T x_i)$ \\
\\
Since $-y_i w^T x_i$ is linear and that the composition of a convex and linear function is convex, g(w) is convex. \\
To expand further, $f(w) = \sum_{i=1}^n g(w)$, where f(w) is a sum of convex functions. \\
To conclude $f(w)$ is then convex.}
}

Hint for Part 5:x`
this function may seem non-convex since it contains $\log(z)$ and $\log$ is concave, but there is a flaw in that reasoning: for example $\log(\exp(z))=z$ is convex despite containing a $\log$. To show convexity, it may be helpful to show that $\log(1+\exp(z))$ is convex, which can be done by computing the second derivative. It may simplify matters to note that $\frac{\exp(z)}{1+\exp(z)} = \frac{1}{1+\exp(-z)}$.

\pagebreak

\section{Linear and Nonlinear Regression}


If you run the script \emph{example\_nonLinear}, it will:
\enum{
\item Load a one-dimensional regression dataset.
\item Fit a least-squares linear regression model.
\item Report the training error.
\item Report the test error (on a dataset not used for training).
\item Draw a figure showing the training data and what the linear model looks like.
}
Unfortunately, this is an awful model of the data. The average squared training error on the data set is over 28000 (as is the test error), and the figure produced by the demo confirms that the predictions are usually nowhere near the training data:
\centerfig{.5}{leastSquares}


\subsection{Linear Regresion with Bias Variable}

The y-intercept of this data is clearly not zero (it looks like it's closer to $200$), so we should expect to improve performance by adding a \emph{bias} variable, so that our model is
\[
y_i = w^Tx_i + w_0.
\]
instead of
\[
y_i = w^Tx_i.
\]
\blu{Write a new function, \emph{leastSquaresBias}, that has the same input/model/predict format as the \emph{leastSquares} function, but that adds a \emph{bias} variable $w_0$. Hand in your new function, the updated plot, and the updated training/test error.}

\ans{Updated training error = 3551.346, Updated test error = 3393.869\\
\\
\red Recall to add the graph and function here}

Hint: recall that adding a bias $w_0$ is equivalent to adding a column of ones to the matrix $X$. Don't forget that you need to do the same transformation in the \emph{predict} function.


\pagebreak

\subsection{Linear Regression with Polynomial Basis}

Adding a bias variable improves the prediction substantially, but the model is still problematic because the target seems to be a \emph{non-linear} function of the input. Write a new function, \emph{leastSquaresBasis(x,y,p)}, that takes a data vector $x$ (i.e., assuming we only have one feature) and the polynomial order $p$. The function should perform a least squares fit based on a matrix $Z$ where each of its rows contains the values $(x_{i})^j$ for $j=0$ up to $p$. E.g., \emph{leastSquaresBasis(x,y,3)} should form the matrix
\[
Z =
\left[\begin{array}{cccc}
1 & x_1 & (x_1)^2 & (x_1)^3\\
1 & x_2 & (x_2)^2 & (x_2)^3\\
\vdots\\
1 & x_n & (x_n)^2 & (x_N)^3\\
\end{array}
\right],
\]
and fit a least squares model based on it.
\blu{Hand in the new function, and report the training and test error for $p = 0$ through $p= 10$. Explain the effect of $p$ on the training error and on the test error.}

\ans{Squared train Error of p=1 with least squares: 3551.346\\
Squared test Error of p=1 with least squares: 3393.869\\
Squared train Error of p=2 with least squares: 2167.992\\
Squared test Error of p=2 with least squares: 2480.725\\
Squared train Error of p=3 with least squares: 252.046\\
Squared test Error of p=6 with least squares: 246.005\\
Squared train Error of p=7 with least squares: 247.011\\
Squared test Error of p=7 with least squares: 242.888\\
Squared train Error of p=8 with least squares: 241.306\\
Squared test Error of p=8 with least squares: 245.966\\
Squared train Error of p=9 with least squares: 235.762\\
Squared test Error of p=9 with least squares: 259.296\\
Squared train Error of p=10 with least squares: 235.074\\
Squared test Error of p=10 with least squares: 256.300\\
\\
The training error continuously decreases as the p value decreases. However, the test error decreases and hits a local minimum at p=7. which it then increases from there. This is due to the fact that p will overfit and cause the curve to fit perfectly to the training points, while the test data would still follow the general trend, which is no longer fitting if p continously increase.}

Note: for this question you should assume that you only have one feature ($d=1$). We will discuss polynomial bases with more input features later in the course.

Hints: To keep the code simple and reduce the chance of having errors, you may want to write a new function \emph{polyBasis} that you can use for transforming both the training and testing data.

\pagebreak

\subsection{Gaussian RBFs}

A popular alternative to using polynomials is to use Gaussian radial basis functions, with one basis function centered on each training example. For a generic feature $\tilde{x}_i$, the transformed feature vector with this choise is given by
\[
\tilde{z}_i = \mat{\underbrace{\exp\left(-\frac{(\tilde{x}_i - x_1)^2}{2\sigma^2}\right)}_\text{feature 1} & \underbrace{\exp\left(-\frac{(\tilde{x}_i - x_2)^2}{2\sigma^2}\right)}_\text{feature 2} & \cdots & \underbrace{\exp\left(-\frac{(\tilde{x}_i - x_n)^2}{2\sigma^2}\right)}_\text{feature $n$}},
\]
where $\sigma^2$ is a hyper-parameter. With $n$ training examples, this generates a set of $n$ features based on a transformation of the distance between the example and each training example. \blu{Hand in a function implementing least squares under this basis, and the plots obtained by $\sigma=10$, $\sigma=1$, and $\sigma=0.1$. How does the value of $\sigma^2$ affect the fundamental trade-off?}
\ans{Squared train Error with gaussianRBF of variance 0.1: 0.797 \\
Squared test Error with gaussianRBF of sigma 0.1: 973886.891 \\
Squared train Error with gaussianRBF of sigma 1.0: 41.787 \\
Squared test Error with gaussianRBF of sigma 1.0: 3426.652 \\ 
Squared train Error with gaussianRBF of sigma 10.0: 250.502 \\
Squared test Error with gaussianRBF of sigma 10.0: 257.218 \\
\\
As the value of $\sigma$ increases, the train error increases, however, on the other hand, the test error decreases.}


\pagebreak

\section{Robust Regression and Gradient Descent}

The script \emph{example\_outliers} loads a one-dimensional regression dataset that has a non-trivial number of `outlier' data points. These points do not fit the general trend of the rest of the data, and pull the least squares model away from the main downward trend that most data points exhibit:
\centerfig{.7}{outliers}



\pagebreak

\subsection{Weighted Least Squares in One Dimension}

One of the most common variations on least squares is \emph{weighted} least squares. In this formulation, we have a weight $v_i$ for every training example. To fit the model, we minimize the weighted squared error,
\[
f(w) =  \frac{1}{2}\sum_{i=1}^n v_i(w^Tx_i - y_i)^2.
\]
In this formulation, the model focuses on making the error small for examples $i$ where $v_i$ is high. Similarly, if $v_i$ is low then the model allows a larger error.

Write a model function, \emph{weightedLeastSquares(X,y,v)}, that implements this model (note that this can be solved as a linear system).
Apply this model to the data containing outliers, setting $v_i = 1$ for the first $400$ data points and $v_i = 0.1$ for the last $100$ data points (which are the outliers). \blu{Hand in your function and the updated plot}.

\ans{}


\pagebreak

\subsection{Smooth Approximation to the L1-Norm}

Unfortunately, we typically do not know the identities of the outliers. In situations where we suspect that there are outliers, but we do not know which examples are outliers, it makes sense to use a loss function that is more robust to outliers. In class, we discussed using the Huber loss,
\[
f(w) = \sum_{i=1}^n h(w^Tx_i  -y_i),
\]
where
\[
h(r_i) =
\begin{cases}
\half r_i^2 & \text{for $|r_i| \leq \epsilon$}\\
\epsilon(|r_i| - \half \epsilon) & \text{otherwise}
\end{cases}.
\]
This is less sensitive to outliers than least squares, although it can no longer be minimized by solving a linear system.
 \blu{Derive
 the gradient $\nabla f$ of this function with respect to $w$. You should show your work but you do not have to express the final result in matrix notation.}
 Hint: you can start by computing the derivative of $h$ with respect to $r_i$ and then get the gradient using the chain rule. You can use sgn$(r_i)$ as a function that returns $1$ if $r_i$ is positive and $-1$ if it is negative.

\ans{Since $r_i = (w^T x_i - y_i), \\
\\
f(r_i(w)) =
\begin{cases}
\sum_{i=1}^n \half r_i^2(r_i) & \text{for $|r_i| \leq \epsilon$}\\
\sum_{i=1}^n \epsilon(|r_i| - \half \epsilon) (r_i) & \text{otherwise}
\end{cases} \\
\\
f'(w_j) = \sum_{i=1}^n f'(r_i(w_j)) \cdot r_i'(w_j) \\
\\
f'(r_i(w_j)) = 
\begin{cases}
\frac{3}{2} r_i^2 & \text{for $|r_i| \leq \epsilon$}\\
\text{sgn}(r_i) 2 \epsilon r_i -  \half \epsilon^2 & \text{otherwise}
\end{cases}
\\\\
r_i'(w_j) = x_{ij}
\\\\
f'(w_j) =  \sum_{i=1}^n
\begin{cases}
\frac{3}{2} r_i^2 * x_{ij} & \text{for $|r_i| \leq \epsilon$}\\
(\text{sgn}(r_i) 2 \epsilon r_i -  \half \epsilon^2) * x_{ij} & \text{otherwise}
\end{cases}$
\\\\
For $\nabla f(w)$, simply make it a d x 1 matrix where row j contains $f'(w_j)$.}
\pagebreak 

\subsection{Robust Regression}

The function \emph{example\_gradient} is the same as \emph{example\_outlier}, except that it fits the least squares model using a \emph{gradient descent} method. You'll see that it produces the same fit as we obtained using the normal equations.

The typical input to a gradient method is a function that, given $w$, returns $f(w)$ and $\nabla f(w)$. See \emph{funObj} in the \emph{leastSquaresGradient} function for an example. Note that \emph{leastSquaresGradient} also has a numerical check that the gradient code is approximately correct, since implementing gradients is often error-prone.\footnote{Though sometimes the numerical gradient checker itself can be wrong. For a lot more on numerical differentiation you can take CPSC 303.}

An advantage of gradient-based strategies is that they are able to solve problems that do not have closed-form solutions, such as the formulation from the previous section. The function \emph{robustRegression} has most of the implementation of a gradient-based strategy for fitting the Huber regression model. The only part missing is the function and gradient calculation inside the \emph{funObj} code. \blu{Modify this function to implement the objective function and gradient based on the Huber loss (from the previous section). Hand in your code, as well as the plot obtained using this robust regression approach with $\epsilon = 1$.}

\pagebreak

\section{Very-Short Answer Questions}



\begin{enumerate}
\item Describe a dataset with $k$ clusters where $k$-means cannot find the true clusters.
\ans{A dataset that have true clusters that are non-convex would would not be able to be classified well. This is as clustering uses radius as a distance, thus, it assumes that clusters are circular in fashion, and non-conventionally shaped clusters would not be able to be clustered correctly.}
\item Why do we need random restarts for $k$-means but not for density-based clustering?
\ans{This is to counter the sensitivity of initialization of the model.}
\item Why is it not a good idea to create an ensemble out of multiple $k$-means runs with random restarts and, for each example, output the mode of the label assignments (voting)?
\ans{There might be mismatch in the classifications. For example, in a 2 true-cluster data, a model might identify the left cluster as "1", but another model might identify it as "2". Thus, this mismatch might even be detrimental to the accuracy.}
\item For each outlier detection method below, list an example method and a problem with identifying outliers using this method:
\begin{itemize}
\item Model-based outlier detection.
\ans{An example method would be using z-normalization to identify. A simple way would be to consider outliers with a high absolute z-score to be an outlier. However, an issue with this could be non-uni-modal data, where there might be double peaks. This would skew the mean of the data, causing the extreme ends of each peak to be considered as outliers.}
\item Graphical-based outlier detection.
\ans{An example method would be using scatterplot graphs and have a person to determine if the data is an outlier. However, this is only limited to 2 features at a time and would be too time consuming to complete if there are many features.}
\item Supervised outlier detection.
\ans{An example method would be to use supervised machine learning to determine. However, this would assume that outliers have already been identified before and the same pattern follows.}
\end{itemize}
\item Why do we minimize $\frac{1}{2}\sum_{i=1} ^n (wx_i-y_i)^2$ instead of the actual mean squared error $\frac{1}{n}\sum_{i=1}^n (wx_i-y_i)^2$ in (1D) least squares?
\ans{Both of the functions have the same minimas and produces the same w. The reason that 1/2 is preferred over 1/n is as it would be an easier function to calculate, given that the derivative will multiply the first value by 2.}
\item Give an example of a feature matrix $X$ for which the least squares problem \emph{cannot} be solved as $w = (X^\top X)^{-1}(X^\top y)$.
\ans{It would be when $X^T X$ is not invertible. An example feature matrix would be [1 1, 1 1].}
\item Why do we typically add a column of $1$ values to $X$ when we do linear regression? Should we do this if we're using decision trees?
\ans{This is to introduce a y-intercept to the graph. This gives the perception of having the same weight for all features, making it a constant y-intercept. Decision tree should not have this column as they split by conditions, thus having the same column of 1 values will not help.}
\item When should we consider using gradient descent to approximate the solution to the least squares problem instead of exactly solving it with the closed form solution?
\ans{When the number of features is large. It will help to cut down on the training time needed.}
\item If a function is convex, what does that say about stationary points of the function? Does convexity imply that a stationary points exists?
\ans{It implies that the stationary point would be the global minimum. However, convexity does not imply that a stationary point will exist. For instance, in a straight line equation, there is no stationary point.}
\item For robust regression based on the L1-norm error, why can't we just set the gradient to 0 and solve a linear system? In this setting, why we would want to use a smooth approximation to the absolute value?
\ans{The L1-norm is non-differentiable, thus attempting to find 0 would not be possible as the minima have a discontinuous point. A smooth approximation allows for the function to be differentiable and solvable.}
\item What is the problem with having too small of a learning rate in gradient descent?
\ans{This would cause the model to have too long of a time to train.}
\item What is the problem with having too large of a learning rate in gradient descent?
\ans{This might cause the gradient descent to never be able to converge. This is as it would amplify the gradient by too much and cause it to move past the convergent point.}
\end{enumerate}




\end{document}
